# askTemoc (Person B)

## File Structure

```
asktemoc/
â”œâ”€â”€ ingest_stub/
â”‚   â””â”€â”€ app.py
â”œâ”€â”€ embed_stub/
â”‚   â””â”€â”€ app.py
â”œâ”€â”€ query_stub/
â”‚   â””â”€â”€ app.py
â”œâ”€â”€ docker-compose.yml
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md  
```

High-level split (4 people)

Person A â€” Ingestion & Parsing (Ingest Team)

Web scrapers / document loaders, OCR pipeline, chunking, document metadata, raw-document store (SQLite).

Person B â€” Embeddings & Vector Store (Index Team)

Embedding pipeline, local vector DB (Chroma/FAISS for MVP), vector index schema, vector search API.

Person C â€” Retrieval & LLM Orchestration (RAG/LLM Team)

Retrieval logic (hybrid later), reranker integration, Langchain chains that call Ollama (or remote model), generation & prompt engineering, chat history manager.

Person D â€” Infra, Evaluation, QA & Dashboard (Ops/Eval Team)

CI, Docker/dev environments, OpenAPI/contract tests, RAG evaluation pipeline (DeepEval/RAGAS), admin dashboard endpoints/CRUD, deployment docs.

## ğŸ¯ Goal (Person B)

Build a microservice that:

### ğŸ”„ Input Processing
- Takes in text chunks via an API endpoint

### ğŸ¤– Embedding Generation  
- Converts text to embedding vectors using models like:
  - `all-MiniLM-L6-v2` (lightweight, fast)
  - `bge-m3` (multilingual, high performance)

### ğŸ’¾ Vector Storage
- Stores embeddings in local vector databases:
  - **Chroma** (recommended for simplicity)
  - **FAISS** (Facebook's high-performance library)

### ğŸ” Search Capabilities
- Supports similarity search queries
- Returns most relevant text chunks based on query vectors
